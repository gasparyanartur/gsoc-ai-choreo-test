{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (55, 4866, 3)\n",
      "dtype float64\n",
      "max 1.9273922443389893\n",
      "min -6.332875728607178\n",
      "1q -1.006934016942978\n",
      "2q -0.4554957449436188\n",
      "3q -0.042966997250914574\n",
      "mean -0.6010307200624114\n",
      "std 0.8776853231783209\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "data_path = Path(\"data/mariel_chunli.npy\")\n",
    "data_raw = np.load(data_path)\n",
    "\n",
    "def inspect_data(data):\n",
    "    print(\"shape\", data.shape)\n",
    "    print(\"dtype\", data.dtype)\n",
    "    print(\"max\", data.max())\n",
    "    print(\"min\", data.min())\n",
    "    print(\"1q\", np.percentile(data, 25))\n",
    "    print(\"2q\", np.percentile(data, 50))\n",
    "    print(\"3q\", np.percentile(data, 75))\n",
    "    print(\"mean\", data.mean())\n",
    "    print(\"std\", data.std())\n",
    "\n",
    "inspect_data(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taken from https://github.com/mariel-pettee/choreo-graph/blob/main/functions/load_data.py\n",
    "\n",
    "ALL_POINT_LABELS = ['ARIEL', 'C7', 'CLAV', 'LANK', 'LBHD', 'LBSH', 'LBWT', 'LELB', 'LFHD', 'LFRM', 'LFSH', 'LFWT', 'LHEL', 'LIEL', 'LIHAND', 'LIWR', 'LKNE', 'LKNI', 'LMT1', 'LMT5', 'LOHAND', 'LOWR', 'LSHN', 'LTHI', 'LTOE', 'LUPA', 'LabelingHips', 'MBWT', 'MFWT', 'RANK', 'RBHD', 'RBSH', 'RBWT', 'RELB', 'RFHD', 'RFRM', 'RFSH', 'RFWT', 'RHEL', 'RIEL', 'RIHAND', 'RIWR', 'RKNE', 'RKNI', 'RMT1', 'RMT5', 'ROHAND', 'ROWR', 'RSHN', 'RTHI', 'RTOE', 'RUPA', 'STRN', 'SolvingHips', 'T10']    \n",
    "BAD_LABELS = ['SolvingHips', 'LabelingHips']\n",
    "POINT_LABELS = [label for label in ALL_POINT_LABELS if label not in BAD_LABELS]\n",
    "NUM_GROUPS = len(POINT_LABELS)\n",
    "\n",
    "skeleton_lines = [\n",
    "#     ( (start group), (end group) ),\n",
    "    ('LHEL', 'LTOE',), # toe to heel\n",
    "    ('RHEL', 'RTOE',),\n",
    "    ('LMT1', 'LMT5',), # horizontal line across foot\n",
    "    ('RMT1', 'RMT5',),   \n",
    "    ('LHEL', 'LMT1',), # heel to sides of feet\n",
    "    ('LHEL', 'LMT5',),\n",
    "    ('RHEL', 'RMT1',),\n",
    "    ('RHEL', 'RMT5',),\n",
    "    ('LTOE', 'LMT1',), # toe to sides of feet\n",
    "    ('LTOE', 'LMT5',),\n",
    "    ('RTOE', 'RMT1',),\n",
    "    ('RTOE', 'RMT5',),\n",
    "    ('LKNE', 'LHEL',), # heel to knee\n",
    "    ('RKNE', 'RHEL',),\n",
    "    ('LFWT', 'RBWT',), # connect pelvis\n",
    "    ('RFWT', 'LBWT',), \n",
    "    ('LFWT', 'RFWT',), \n",
    "    ('LBWT', 'RBWT',),\n",
    "    ('LFWT', 'LBWT',), \n",
    "    ('RFWT', 'RBWT',), \n",
    "    ('LFWT', 'LTHI',), # pelvis to thighs\n",
    "    ('RFWT', 'RTHI',), \n",
    "    ('LBWT', 'LTHI',), \n",
    "    ('RBWT', 'RTHI',), \n",
    "    ('LKNE', 'LTHI',), \n",
    "    ('RKNE', 'RTHI',), \n",
    "    ('CLAV', 'LFSH',), # clavicle to shoulders\n",
    "    ('CLAV', 'RFSH',), \n",
    "    ('STRN', 'LFSH',), # sternum & T10 (back sternum) to shoulders\n",
    "    ('STRN', 'RFSH',), \n",
    "    ('T10', 'LFSH',), \n",
    "    ('T10', 'RFSH',), \n",
    "    ('C7', 'LBSH',), # back clavicle to back shoulders\n",
    "    ('C7', 'RBSH',), \n",
    "    ('LFSH', 'LBSH',), # front shoulders to back shoulders\n",
    "    ('RFSH', 'RBSH',), \n",
    "    ('LFSH', 'RBSH',),\n",
    "    ('RFSH', 'LBSH',),\n",
    "    ('LFSH', 'LUPA',), # shoulders to upper arms\n",
    "    ('RFSH', 'RUPA',), \n",
    "    ('LBSH', 'LUPA',), \n",
    "    ('RBSH', 'RUPA',), \n",
    "    ('LIWR', 'LIHAND',), # wrist to hand\n",
    "    ('RIWR', 'RIHAND',),\n",
    "    ('LOWR', 'LOHAND',), \n",
    "    ('ROWR', 'ROHAND',),\n",
    "    ('LIWR', 'LOWR',), # across the wrist \n",
    "    ('RIWR', 'ROWR',), \n",
    "    ('LIHAND', 'LOHAND',), # across the palm \n",
    "    ('RIHAND', 'ROHAND',), \n",
    "    ('LFHD', 'LBHD',), # draw lines around circumference of the head\n",
    "    ('LBHD', 'RBHD',),\n",
    "    ('RBHD', 'RFHD',),\n",
    "    ('RFHD', 'LFHD',),\n",
    "    ('LFHD', 'ARIEL'), # connect circumference points to top of head\n",
    "    ('LBHD', 'ARIEL'),\n",
    "    ('RBHD', 'ARIEL'),\n",
    "    ('RFHD', 'ARIEL'),\n",
    "]\n",
    "\n",
    "POINT_IDXS = {label: i for i, label in enumerate(POINT_LABELS)}\n",
    "EDGES = [(POINT_IDXS[start], POINT_IDXS[end]) for start, end in skeleton_lines]\n",
    "EDGES = np.array(EDGES)\n",
    "EDGES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betternot_and_retrograde 159\n",
      "beyond 97\n",
      "chunli 188\n",
      "honey 124\n",
      "knownbetter 284\n",
      "penelope 150\n"
     ]
    }
   ],
   "source": [
    "# For some reason, there are some frames that are the same as the previous frame\n",
    "\n",
    "START_IDX = {}\n",
    "\n",
    "for fn in [\"betternot_and_retrograde\", \"beyond\", \"chunli\", \"honey\", \"knownbetter\", \"penelope\"]:\n",
    "    p = Path(f\"data/mariel_{fn}.npy\")\n",
    "    data_i = np.load(p)\n",
    "    data_i = data_i.swapaxes(0, 1)\n",
    "    for i in range(1, data_i.shape[0]):\n",
    "        if (data_i[i] != data_i[i-1]).all():\n",
    "            print(fn, i)\n",
    "            START_IDX[fn] = i\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First dimension is the edge group.\n",
    "Second is the frames for a given clip.\n",
    "Third is the XYZ coordinates in 3D space. \n",
    "\n",
    "We know the edge groups from [Pettee's previous project](https://github.com/mariel-pettee/choreography/blob/master/functions/functions.py), and we also know that groups 27 and 54 are bad edge groups. \n",
    "\n",
    "We reshape and mask those out. Additionally, we preprocess the data to be in the range -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (4866, 53, 3)\n",
      "dtype float64\n",
      "max 1.0\n",
      "min -1.0\n",
      "1q -0.2738534668068461\n",
      "2q 0.3330744535997223\n",
      "3q 0.5042046208517127\n",
      "mean 0.15681260326528199\n",
      "std 0.4136803405246239\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data, normalize=True):\n",
    "    bad_groups = [i for i, group in enumerate(ALL_POINT_LABELS) if group in BAD_LABELS]\n",
    "    group_mask = np.ones(data.shape[0], dtype=bool)\n",
    "    group_mask[bad_groups] = False\n",
    "    data = data[group_mask]\n",
    "\n",
    "    data = data.swapaxes(0, 1)\n",
    "\n",
    "    if normalize:\n",
    "        min_val = data.min()\n",
    "        max_val = data.max()\n",
    "        data = (data - min_val) / (max_val - min_val) * 2 - 1\n",
    "\n",
    "    data[:, :,  2] *= -1        # invert z axis\n",
    "\n",
    "    return data\n",
    "\n",
    "data = preprocess_data(data_raw, normalize=True)\n",
    "inspect_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib QtAgg\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "class FigureAnimation:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        self.fig = plt.figure()\n",
    "        self.ax = self.fig.add_subplot(projection='3d')\n",
    "\n",
    "        self.ax.set_xlim(data[:, :, 0].min(), data[:, :, 0].max())\n",
    "        self.ax.set_ylim(data[:, :, 1].min(), data[:, :, 1].max())\n",
    "        self.ax.set_zlim(data[:, :, 2].min(), data[:, :, 2].max())\n",
    "\n",
    "        self.scatter_plot = None\n",
    "        self.lineplots = None\n",
    "\n",
    "    def start(self, start_frame: int = 0, end_frame: int = -1, framerate: int = 20):\n",
    "        if end_frame == -1:\n",
    "            end_frame = self.data.shape[0]\n",
    "\n",
    "        def setup():\n",
    "            snapshot = self.data[start_frame]\n",
    "            self.scatter_plot = self.ax.scatter(snapshot[:, 0], snapshot[:, 1], snapshot[:, 2])\n",
    "\n",
    "            start_edges, end_edges = EDGES[:, 0], EDGES[:, 1]\n",
    "            lines = np.stack([snapshot[start_edges], snapshot[end_edges]], axis=1)\n",
    "            \n",
    "            self.lineplots = [self.ax.plot(line[:, 0], line[:, 1], line[:, 2], color='black')[0] for line in lines]\n",
    "\n",
    "            return self.scatter_plot, *self.lineplots\n",
    "\n",
    "        def update(frame):\n",
    "            snapshot = self.data[int(frame)]\n",
    "            self.scatter_plot._offsets3d = (snapshot[:, 0], snapshot[:, 1], snapshot[:, 2])\n",
    "\n",
    "            start_edges, end_edges = EDGES[:, 0], EDGES[:, 1]\n",
    "            lines = np.stack([snapshot[start_edges], snapshot[end_edges]], axis=1)\n",
    "            for lineplot, line in zip(self.lineplots, lines):\n",
    "                lineplot.set_data(line[:, 0], line[:, 1])\n",
    "                lineplot.set_3d_properties(line[:, 2]) \n",
    "\n",
    "            self.fig.canvas.draw_idle()\n",
    "\n",
    "            if frame == end_frame - 1:\n",
    "                self.ani.event_source.stop()\n",
    "\n",
    "            return self.scatter_plot, *self.lineplots\n",
    "\n",
    "        self.ani = FuncAnimation(self.fig, update, init_func=setup, frames=range(start_frame, end_frame), interval=1000/framerate, blit=False, cache_frame_data=False)\n",
    "        self.fig.show()\n",
    "\n",
    "\n",
    "#fig_anim = FigureAnimation(data)\n",
    "#fig_anim.start(start_frame=150, framerate=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4866, 53, 3), (968, 30, 53, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_data(data, window_size, stride):\n",
    "    # data: (n_frames, n_points, 3)\n",
    "    n_frames = data.shape[0]\n",
    "    n_points = data.shape[1]\n",
    "    n_channels = data.shape[2]\n",
    "\n",
    "    n_windows = (n_frames - window_size) // stride + 1\n",
    "    windows = np.zeros((n_windows, window_size, n_points, n_channels))\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        start = i * stride\n",
    "        end = start + window_size\n",
    "        windows[i] = data[start:end]\n",
    "\n",
    "    return windows\n",
    "\n",
    "data.shape, split_data(data, window_size=30, stride=5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2148, 30, 53, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data_with_pattern(data_dir: Path, filenamn_pattern: str = \"mariel\", window_size: int = 30, stride: int = 5, normalize: bool = True):\n",
    "    all_data = []\n",
    "\n",
    "    for data_path in data_dir.glob(filenamn_pattern):\n",
    "        data_raw = np.load(data_path)\n",
    "        data = preprocess_data(data_raw, normalize=normalize)\n",
    "        \n",
    "        name = data_path.stem[len(\"mariel_\"):]\n",
    "        start_idx = START_IDX.get(name, 0)\n",
    "        data = data[start_idx:]\n",
    "\n",
    "        windows = split_data(data, window_size, stride)\n",
    "\n",
    "        all_data.append(windows)\n",
    "\n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    return all_data\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "all_data = get_data_with_pattern(data_dir, filenamn_pattern=\"mariel_better*\", window_size=30, stride=5, normalize=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig_anim = FigureAnimation(all_data[3])\n",
    "#fig_anim.start(framerate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_around_z(data, radians: float):\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(radians), -np.sin(radians), 0],\n",
    "        [np.sin(radians), np.cos(radians), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    return np.einsum(\"ijk,kl->ijl\", data, rotation_matrix)\n",
    "\n",
    "\n",
    "rotated_data = rotate_around_z(all_data[3], np.pi/2)\n",
    "#fig_anim = FigureAnimation(rotated_data)\n",
    "#fig_anim.start(framerate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rotation_matrices(radians: Tensor, device) -> Tensor:\n",
    "    batch_size = radians.shape[0]\n",
    "    rotation_matrices = torch.zeros(batch_size, 3, 3, device=device)\n",
    "\n",
    "    rotation_matrices[:, 0, 0] = torch.cos(radians)\n",
    "    rotation_matrices[:, 0, 1] = -torch.sin(radians)\n",
    "    rotation_matrices[:, 1, 0] = torch.sin(radians)\n",
    "    rotation_matrices[:, 1, 1] = torch.cos(radians)\n",
    "    rotation_matrices[:, 2, 2] = 1\n",
    "\n",
    "    return rotation_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rotate_around_z(data: Tensor, radians: Tensor) -> Tensor:\n",
    "    assert data.shape[0] == radians.shape[0]\n",
    "\n",
    "    rotation_matrix = create_rotation_matrices(radians, data.device)\n",
    "    return torch.einsum(\"ijk,ikl->ijl\", data, rotation_matrix)\n",
    "\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        # Since we only care about poses, not sequences, we can flatten the first two dimensions\n",
    "        self.data = self.data.view(-1, *self.data.shape[2:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose Embedding\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class PoseEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Imput: (N, num_groups, 3)\n",
    "        # Output: (N, embedding_dim)\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(NUM_GROUPS * 3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, NUM_GROUPS * 3)\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class PoseDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Imput: (N, embedding_dim)\n",
    "        # Output: (N, num_groups, 3)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, NUM_GROUPS * 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, NUM_GROUPS, 3)\n",
    "        return x\n",
    "\n",
    "class PoseAutoencoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = PoseEncoder(embedding_dim)\n",
    "        self.decoder = PoseDecoder(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PoseAutoencoderModel(LightningModule):\n",
    "    def __init__(self, embedding_dim, lr=1e-3, train_frac=0.8, aug_random_noise: float = 0.0, aug_random_rotation: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model = PoseAutoencoder(embedding_dim)\n",
    "        self.lr = lr\n",
    "\n",
    "        self.aug_random_noise = aug_random_noise\n",
    "        self.aug_random_rotation = aug_random_rotation\n",
    "\n",
    "        self.all_data = PoseDataset(all_data)\n",
    "        n_samples = len(self.all_data)\n",
    "        self.train_data, self.val_data = torch.utils.data.random_split(self.all_data, [int(n_samples * train_frac), n_samples - int(n_samples * 0.8)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch: Tensor, batch_idx):\n",
    "        x = batch\n",
    "\n",
    "        if self.aug_random_rotation > 0:\n",
    "            radians = (torch.rand(x.shape[0], device=x.device) - 0.5) * self.aug_random_rotation * torch.tensor(np.pi)\n",
    "            x = batch_rotate_around_z(x, radians)\n",
    "\n",
    "        z = self.model.encoder(x)\n",
    "\n",
    "        if self.aug_random_noise > 0:\n",
    "            noise = torch.randn_like(x) * self.aug_random_noise\n",
    "            z = z + noise\n",
    "\n",
    "        x_hat = self.model.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=128, shuffle=True, num_workers=8)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat = self.model(x)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=128)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat = self.model(x)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PoseAutoencoderModel(embedding_dim=64)\n",
    "logger = TensorBoardLogger(\"logs/poseae\", name=\"pose_autoencoder\")\n",
    "trainer = Trainer(max_epochs=100, logger=logger)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_batch = torch.tensor(all_data[0], dtype=torch.float32).unsqueeze(0)\n",
    "ex_batch_hat = model(ex_batch)\n",
    "\n",
    "fig_anim = FigureAnimation(ex_batch_hat.detach().numpy())\n",
    "fig_anim.start(framerate=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, pose_encoder: PoseEncoder):\n",
    "        super().__init__()\n",
    "\n",
    "        # Imput: (N, seq_len, num_groups, 3)\n",
    "        # Output: (N, embedding_dim)\n",
    "        self.pose_encoder = pose_encoder\n",
    "\n",
    "        # First, we encode each pose in the sequence, giving us a tensor of shape (N, seq_len, embedding_dim)\n",
    "        # Then, we pass this tensor through a Transformer encoder, which gives us a tensor of shape (N, embedding_dim)\n",
    "        self.positional_encoder = nn.Embedding(len(POINT_LABELS), embedding_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim, nhead=8, batch_first=True\n",
    "            ),\n",
    "            num_layers=6,\n",
    "            mask_check=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        x = x.view(batch_size * seq_len, NUM_GROUPS, 3)\n",
    "        x = self.pose_encoder(x)\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        x = x + self.positional_encoder.weight\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeqDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, pose_decoder: PoseDecoder):\n",
    "        super().__init__()\n",
    "\n",
    "        # Imput: (N, embedding_dim)\n",
    "        # Output: (N, seq_len, num_groups, 3)\n",
    "        self.pose_decoder = pose_decoder\n",
    "\n",
    "        # Dense layers to map the transformer output to the pose decoder input\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, NUM_GROUPS * 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.view(batch_size * seq_len, -1)\n",
    "        x = self.pose_decoder.decoder(x)\n",
    "        x = x.view(batch_size, seq_len, NUM_GROUPS, 3)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeqAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim, pose_encoder: PoseEncoder, pose_decoder: PoseDecoder\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = SeqEncoder(embedding_dim, pose_encoder)\n",
    "        self.decoder = SeqDecoder(embedding_dim, pose_decoder)\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "        return self.decoder(self.encoder(x), seq_len)\n",
    "\n",
    "\n",
    "class SeqAutoencoderModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        lr=1e-3,\n",
    "        train_frac=0.8,\n",
    "        seq_len=30,\n",
    "        aug_random_noise: float = 0.0,\n",
    "        aug_random_rotation: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = SeqAutoencoder(\n",
    "            embedding_dim, PoseEncoder(embedding_dim), PoseDecoder(embedding_dim)\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.aug_random_noise = aug_random_noise\n",
    "        self.aug_random_rotation = aug_random_rotation\n",
    "\n",
    "        self.all_data = PoseDataset(all_data)\n",
    "        n_samples = len(self.all_data)\n",
    "        self.train_data, self.val_data = torch.utils.data.random_split(\n",
    "            self.all_data,\n",
    "            [int(n_samples * train_frac), n_samples - int(n_samples * 0.8)],\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, self.seq_len)\n",
    "\n",
    "    def training_step(self, batch: Tensor, batch_idx):\n",
    "        x = batch\n",
    "        z = self.model.encoder(x)\n",
    "\n",
    "        if self.aug_random_noise > 0:\n",
    "            noise = torch.randn_like(x) * self.aug_random_noise\n",
    "            z = z + noise\n",
    "\n",
    "        x_hat = self.model.decoder(z, self.seq_len)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat = self.model(x, self.seq_len)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=128)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat = self.model(x, self.seq_len)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model | SeqAutoencoder | 526 K  | train\n",
      "-------------------------------------------------\n",
      "526 K     Trainable params\n",
      "0         Non-trainable params\n",
      "526 K     Total params\n",
      "2.104     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28a37d1ff8d423fb661cb6a08281d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gasparyanartur/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6784x3 and 159x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m logger = TensorBoardLogger(\u001b[33m\"\u001b[39m\u001b[33mlogs/seqae\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[33m\"\u001b[39m\u001b[33mseq_autoencoder\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m trainer = Trainer(max_epochs=\u001b[32m100\u001b[39m, logger=logger)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    538\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    569\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    570\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    571\u001b[39m     ckpt_path,\n\u001b[32m    572\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    573\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    977\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    987\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1024\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1026\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1050\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1052\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1057\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:144\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:433\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    427\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m step_args = (\n\u001b[32m    429\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    432\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:323\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    326\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mSeqAutoencoderModel.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[32m     94\u001b[39m     x = batch\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     x_hat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     loss = nn.functional.mse_loss(x_hat, x)\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m.log(\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mSeqAutoencoder.forward\u001b[39m\u001b[34m(self, x, seq_len)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, seq_len):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, seq_len)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mSeqEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     15\u001b[39m seq_len = x.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     16\u001b[39m x = x.view(-\u001b[32m1\u001b[39m, NUM_GROUPS, \u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpose_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m x = x.view(batch_size, seq_len, -\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m x, _ = \u001b[38;5;28mself\u001b[39m.lstm(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gsoc-ai-choreo-test/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (6784x3 and 159x512)"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SeqAutoencoderModel(embedding_dim=64, seq_len=30)\n",
    "logger = TensorBoardLogger(\"logs/seqae\", name=\"seq_autoencoder\")\n",
    "trainer = Trainer(max_epochs=100, logger=logger)\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
